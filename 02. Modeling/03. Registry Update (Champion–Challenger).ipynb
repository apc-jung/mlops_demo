{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b715cf4-cba7-4408-99e3-3928f7e341a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "from mlflow.exceptions import RestException\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec6079d7-935e-4be6-a7b8-a16a5fd5624b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test = spark.table(\"MLOps.data.test_table\")\n",
    "\n",
    "model_name = \"als_recommendation_model\"\n",
    "experiment_path = \"/Workspace/Users/jung@ap-com.co.jp/mlops_demo_model/als_recommendation\"\n",
    "experiment = mlflow.get_experiment_by_name(experiment_path)\n",
    "runs_df = mlflow.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id]\n",
    ")\n",
    "training_runs = runs_df[runs_df[\"tags.mlflow.runName\"] == \"training\"]\n",
    "latest_training_run = training_runs.sort_values(\n",
    "    by=\"start_time\", ascending=False\n",
    ").iloc[0]\n",
    "model_uri = f\"runs:/{latest_training_run.run_id}/als_model\"\n",
    "\n",
    "challenger = mlflow.spark.load_model(model_uri)\n",
    "\n",
    "try:\n",
    "    champion = mlflow.spark.load_model(f\"models:/{model_name}/Production\")\n",
    "    print(\"Champion model exists in Production\")\n",
    "except Exception as e:\n",
    "    champion = None\n",
    "    print(\"No Champion model found. Challenger will be promoted directly.\", e)\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"interaction_weight\",  \n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "predictions_challenger = challenger.transform(test)\n",
    "challenger_rmse = evaluator.evaluate(predictions_challenger)\n",
    "print(\"Challenger RMSE:\", challenger_rmse)\n",
    "\n",
    "if champion is not None:\n",
    "    predictions_champion = champion.transform(test)\n",
    "    champion_rmse = evaluator.evaluate(predictions_champion)\n",
    "    print(\"Champion RMSE:\", champion_rmse)\n",
    "else:\n",
    "    champion_rmse = None\n",
    "\n",
    "if champion is None or challenger_rmse < champion_rmse:\n",
    "    print(\"Challenger outperforms Champion (or no Champion exists). Promoting...\")\n",
    "\n",
    "    mlflow.register_model(model_uri, model_name)\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "    latest_version = client.get_latest_versions(model_name, stages=[\"None\"])[-1].version\n",
    "\n",
    "    client.transition_model_version_stage(\n",
    "        name=model_name,\n",
    "        version=latest_version,\n",
    "        stage=\"Production\",\n",
    "        archive_existing_versions=True\n",
    "    )\n",
    "    print(\"Challenger promoted to Champion (Production).\")\n",
    "else:\n",
    "    print(\"Challenger did not outperform Champion. No promotion.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03. Registry Update (Championâ€“Challenger)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
